---
title: 'BIAS Blog 1: Right to Fair Representation'
date: 2025-10-24
permalink: /posts/2025/10/blog-post-BIAS-1/
tags:
  - ai-ethics
  - cultural-representation
  - generative-ai
  - south-asia
  - technology
---

*Investigating how generative AI models represent different cultures and how those representations can reinforce bias, power, and inequality.*

---

**The Case Study:**  
[AI’s Regimes of Representation: A Community-Centered Study of Text-to-Image Models in South Asia](https://doi.org/10.21428/2c646de5.2b73d5c2)

---

## Summary of the case study
The case study *“AI’s Regimes of Representation”* investigates how text-to-image (T2I) AI models depict South Asian people and cultures. It shows that these AIs often reproduce old stereotypes and Western-centered views instead of authentic representations. The researchers worked with participants from Pakistan, India, and Bangladesh, who shared how the images made them feel and where AI failed to understand their local identities. The study highlights how generative AI can unintentionally carry colonial biases and shape how entire regions are seen by the world.


## Discussion Questions and My Thoughts
### 1. What does cultural representation mean to you, and how might that affect how you’d judge AI’s representation of your identity?
To me, cultural representation is about being seen for who we really are not a stereotype or a simple version of what someone thinks we are. It’s frustrating when technology or media only shows one side of a culture, especially when that side feels outdated or negative. If I saw an AI showing my culture only through poverty, chaos, or “traditional” images, I’d feel invisible in a way. Good example if you search like south sudan on google it will show you the negative side of it, which that is not full reality. Representation should reflect the reality I know modern, diverse, and full of life not someone else’s version of it.

### 2. What’s the role of small-scale qualitative studies in making AI more ethical?
I think these small, people-centered studies matter most. Numbers and accuracy scores don’t tell you what’s actually felt by the people being represented. Hearing voices from the communities themselves gives a fuller picture of what fairness looks like. Ethical AI needs that human perspective.

### 3. Do you think AI can really be made globally inclusive?
AI can improve, but it’ll never be perfect. The world is too diverse for one model to get everything right. But if companies work directly with people from different cultures — not just collect their data — inclusivity can grow. It’s about shifting power, not just adding more images.

### 4. What can developers do to address the issues raised in the study?
Developers should bring community experts into the process and be transparent about what their models can’t do yet. They should also create feedback systems where users can report harmful or inaccurate outputs. Cultural fairness should be treated like a safety issue — something essential, not optional.

### 5. Since representation changes over time, can that really be “encoded” in AI? 
I don’t think representation can ever be fully coded because culture keeps changing. But AI could evolve through feedback, just like humans do. Instead of freezing cultural data in time, developers could make models that adapt and learn from ongoing community input. Still, we’ll always need real people to guide what’s appropriate and what’s not.

### 6. What can we learn from the history of tech and media to make AI more responsible?
If history teaches us anything, it’s that technology often repeats social inequalities. Photography, film, and now AI — they’ve all shaped who gets seen and who doesn’t. Building responsible AI means learning from those past mistakes and making space for marginalized groups to represent themselves.

## My Discussion Question
**What would it look like if communities built their own AI tools — ones that reflect how we want to be seen, instead of waiting for big tech companies to “get it right”?**

I came up with this because I realized we always talk about fixing bias inside big systems, but rarely about creating new ones that belong to the people they represent. Maybe true fairness means community-built AI. And also this makes me thing of the question, for who?

## Reflection
This case study made me think deeply about how powerful images are, and how much damage biased ones can do. The participants weren’t just pointing out errors; they were calling out how AI repeats old power structures. It reminded me that “fairness” in AI isn’t just about algorithms or datasets — it’s about whose voices count. If AI continues to tell our stories, it should do so with our input and respect. Otherwise, it just becomes another tool for outsiders to define us.

---
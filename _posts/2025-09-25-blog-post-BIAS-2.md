---
title: 'BIAS Blog 2: Harms in Machine Learning'
date: 2025-10-30
permalink: /posts/2025/10/blog-post-BIAS-2/
tags:
  - ai-ethics
  - machine-learning
  - bias
---

Exploring how machine learning systems an unintentionally cause harm, and why we must build AI carefully and fairly.

**Case study:**  
[Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle ](https://mit-serc.pubpub.org/pub/potential-sources-of-harm-throughout-the-machine-learning-life-cycle/release/2)

---
## Summary of the Case Study 
This case study explains how machine learning systems can sometimes cause harm not just through bias in data, but in many other ways too. For example, throughout their life cycle, from data collection to deployment. It identifies seven key sources of harm and encourages developers to be intentional and ethical at every step of building AI systems.

## Discussion 

### 1. Example of harm
One example that stood out is historical bias in hiring software. If a company has historically hired mostly white men for engineering roles, a model trained on that data might continue prioritizing white male applicants, even if qualifications are the same. This isn’t because the data is wrong, but because the history itself is biased and limited.
Another example is representation bias, such as voice assistants that struggle to understand certain English accents or pronunciations. If the training data mostly included American voices, the model won’t work equally well for everyone. I have experienced this firsthand, sometimes I have to repeat what I ask Siri multiple times. It can be frustrating for users.

These examples show how technical systems can mirror social inequalities unless we actively challenge them. With technology now reaching around 73% of the global population, AI should be more diverse and accommodating.

### 2. How this relates to technology I’ve used
When I use platforms like LinkedIn, job recommendations often feel narrow or repetitive, which might reflect aggregation bias. The model seems to assume that everyone with my major or school background wants a certain type of job, instead of recognizing individual career interests. While it can make it easier to find jobs that fit my skills, it might not capture opportunities to challenge myself—because the algorithm only reflects the data it’s fed.
Algorithmic feeds on social media sometimes reinforce stereotypes or reduce visibility for certain communities, reflecting both representation and historical bias. For instance, if I like a certain video on Instagram, I keep getting similar videos, which isn’t necessarily what I want. I’ve also noticed that location affects feeds—when I’m in another country, I mostly see content from that region. Even when sharing Reels with someone nearby, they often continue seeing what I watch.

Thinking about this makes me wonder how many opportunities or voices I’ve missed because an algorithm filtered them out.

### 3. Additional ideas for mitigation
Beyond the suggestions in the case study, I think AI systems should involve community input before deployment. The people most impacted by technology should help shape it—not just engineers or companies.
Transparency also matters. If users knew why a model made a decision about them—like loan approval or job matching—they could challenge mistakes and hold systems accountable. Even simple explanations could reduce harm, provide opportunities, and build trust.

## My Discussion Question:
How should we decide when not to use machine learning at all? For example, are there social decisions (like sentencing, housing, or college admissions) where the existence of historical inequality means AI will always reinforce harm?
**Why I chose this question:**
The case study emphasizes mitigation, but sometimes the ethical choice might be to stop using AI altogether. Not every task should be automated. This question pushes us to think beyond fixing bias and toward asking whether AI is appropriate in the first place.

## Reflection
This assignment helped me see bias in AI as more than just “fixing the dataset.” Every stage—from data collection to deployment—can introduce harm. AI doesn’t operate in a vacuum; it inherits history and shapes the future.
It also made me wonder: are the biases we see in AI similar to the stereotypes around us, and is it healthy to reproduce them in the tech industry?

Fairness requires understanding context, not just optimizing accuracy scores. Ethical AI isn’t only a technical challenge—it’s a social one. Technology doesn’t just reflect society—it shapes it. We share the responsibility to question, improve, and sometimes reject systems that can cause harm.
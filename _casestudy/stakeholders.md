---
permalink: /casestudy/stakeholders/
title: "Stakeholders"
layout: single
---
## 1. Patients (Especially Darker-Skin Patients)
Patients are the most directly affected stakeholders, and their well-being lies at the center of this ethical issue. AI-driven dermatology tools may misdiagnose or under-diagnose lesions in darker-skinned patients due to biased or insufficiently diverse training datasets, potentially delaying treatment for serious conditions like melanoma.

Patients with darker skin already face historical disparities in dermatological care, including underrepresentation in medical research and limited educational focus on diagnosing conditions in darker skin. If AI systems replicate or amplify these inequities, health outcome disparities deepen. Beyond clinical outcomes, such biases threaten patient trust in clinicians, healthcare institutions, and emerging technologies.

AI integration also raises ethical concerns around autonomy, informed consent, and data privacy. As highlighted by Chen et al. (2024), patients should be informed about how AI is used in their care, including risks, benefits, and how their data may be used in algorithm training. Smartphone applications and patient-facing AI platforms may provide earlier detection opportunities but can also give misleading assessments or cause unnecessary anxiety. Ensuring patient understanding and transparency is critical to maintain trust and autonomy.

Research indicates that AI algorithms trained on datasets with insufficient representation of darker skin tones can reduce diagnostic accuracy for these patients, emphasizing the need for diverse, inclusive data to ensure equitable care (Chen et al., 2024; Conger, 2024).

## 2. Doctors and nurses
Clinicians depend on accurate tools to support their diagnostic decisions. AI assistance, as shown in the Stanford study, can improve diagnostic accuracy across practitioners, especially for non-specialists (Conger, 2024). If the AI system is biased, doctors and nurses may unknowingly rely on inaccurate recommendations, especially in fast-paced clinical settings where AI is meant to “speed up” decision-making. This poses several challenges:
- Clinicians may make incorrect diagnoses based on faulty AI output.
- Their professional credibility could be questioned if errors occur.
- They must balance clinical expertise with AI recommendations, which adds cognitive and ethical burden.
Doctors and nurses also experience increasing workloads, and AI tools are often introduced with the promise of reducing workload and improving efficiency. However, biased tools may instead increase their responsibilities—requiring them to double-check AI decisions, handle patient concerns, or navigate unfamiliar algorithmic limitations. Ultimately, clinicians need systems they can trust, not ones that introduce uncertainty or risk.

Indeed, evidence indicates that AI guidance improves sensitivity and specificity for clinicians, particularly for non-specialists, but biased datasets can reduce this benefit for certain patient groups, increasing the burden on clinicians to detect errors and maintain equitable care (Conger, 2024).

## 3. Hospitals and Healthcare Organizations
Hospitals and clinics are responsible for choosing, purchasing, and implementing AI systems. They stand to benefit from AI through:
- Faster patient workflow
- Reduced diagnostic costs
- Improved efficiency and reduced strain on specialists
- Enhanced reputation for technological advancement

However, they also bear significant risks. If an AI tool performs poorly for certain groups, hospitals may face:
- Medical errors that lead to patient harm
- Legal liability
- Loss of public trust
- Damage to institutional reputation

Hospitals must weigh the benefits of innovation against the ethical obligation to ensure patient safety for all demographics. They also have a responsibility to evaluate AI vendors carefully, demand transparency from developers, and enforce equity-focused testing before adoption and make sure it is friendly user.

## 4. AI Companies and Developers
This is the most important part of this innovation. AI developers create and train the algorithms used in medical diagnostics. Their decisions shape what data is included, how it is labeled, how the model is validated, and how performance across demographic groups is reported (or not reported). Companies play a central role in ensuring algorithms are trained on diverse, representative datasets. They must also develop systems that are transparent, explainable, and accountable.

The incentives for AI companies, however, are complex. They face pressure to innovate quickly, release products fast, and satisfy investors, all of which may lead to shortcuts in testing, limited bias analysis, or insufficient reporting of demographic performance. Companies hold the technical knowledge that hospitals may not have, so they also have a responsibility to clearly communicate the limitations, risks, and performance characteristics of their products.

Therefore, developers must ensure datasets are representative and inclusive, as biased training data can directly perpetuate inequities in diagnostic accuracy and patient outcomes (Chen et al., 2024)

## 5. Government and Regulatory Bodies
Government agencies and medical regulators play a critical role in establishing rules for safe, ethical AI use in healthcare. These institutions ensure that:
- Medical devices meet safety standards
- Bias testing is performed before approval
- Companies disclose risk and performance data
- Patient data is protected
- Discriminatory outcomes are prevented

Nevertheless, AI technology has evolved faster than regulatory frameworks. Many governments do not yet require companies to prove demographic fairness or disclose performance gaps by race or skin tone. Regulators must balance innovation with patient protection and have a responsibility to create standards that prevent biased AI from entering clinical practice. Their involvement ensures that the public interest—particularly the protection of vulnerable communities—is prioritized over commercial interests.


[⬅️ Back to Main Case study](/ethics_fall2025/casestudy)
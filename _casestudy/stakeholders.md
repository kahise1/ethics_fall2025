---
permalink: /casestudy/stakeholders/
title: "Stakeholders"
layout: default
---
**STAKEHOLDERS:**
## 1. Patients (Especially Darker-Skin Patients)
Patients are the most directly affected stakeholders among all, and their wellbeing is at the center of this ethical issue. Because this technology is applied to them as subjects. In the context of AI skin cancer diagnostic tools, patients, particularly those with darker skin tones face the highest risk of harm. These tools may misdiagnose or under-diagnose on darker skin due to biases in the training data. For these patients, a false negative could mean delayed treatment for melanoma(a serious type of skin cancer that starts in the pigment-producing cells called melanocytes) or other serious skin cancers, which significantly worsens health outcomes and may even become life threatening.

Moreover, darker skin patients already experience unequal access to dermatological care due to systemic issues, including a historical lack of medical education and research on skin diseases in darker skin. If AI systems repeat or amplify these inequalities, the result is a deepening of existing disparities. Patients also rely heavily on trust—trust in clinicians, in technology, and in the healthcare system as a whole. Biased AI systems risk breaking that trust and further discouraging marginalized groups from seeking timely medical care.

## 2. Doctors and nurses
Clinicians depend on accurate tools to support their diagnostic decisions. If the AI system is biased, doctors and nurses may unknowingly rely on inaccurate recommendations, especially in fast-paced clinical settings where AI is meant to “speed up” decision-making. This poses several challenges:
Clinicians may make incorrect diagnoses based on faulty AI output.
Their professional credibility could be questioned if errors occur.
They must balance clinical expertise with AI recommendations, which adds cognitive and ethical burden.
Doctors and nurses also experience increasing workloads, and AI tools are often introduced with the promise of reducing workload and improving efficiency. However, biased tools may instead increase their responsibilities—requiring them to double-check AI decisions, handle patient concerns, or navigate unfamiliar algorithmic limitations. Ultimately, clinicians need systems they can trust, not ones that introduce uncertainty or risk.

## 3. Hospitals and Healthcare Organizations
Hospitals and clinics are responsible for choosing, purchasing, and implementing AI systems. They stand to benefit from AI through:
	Faster patient workflow
	Reduced diagnostic costs
	Improved efficiency and reduced strain on specialists
	Enhanced reputation for technological advancement

However, they also bear significant risks. If an AI tool performs poorly for certain groups, hospitals may face:
	Medical errors that lead to patient harm
	Legal liability
	Loss of public trust
	Damage to institutional reputation

Hospitals must weigh the benefits of innovation against the ethical obligation to ensure patient safety for all demographics. They also have a responsibility to evaluate AI vendors carefully, demand transparency from developers, and enforce equity-focused testing before adoption and make sure it is friendly user.

## 4. AI Companies and Developers
This is the most important part of this innovation. AI developers create and train the algorithms used in medical diagnostics. Their decisions shape what data is included, how it is labeled, how the model is validated, and how performance across demographic groups is reported (or not reported). Companies play a central role in ensuring algorithms are trained on diverse, representative datasets. They must also develop systems that are transparent, explainable, and accountable.

The incentives for AI companies, however, are complex. They face pressure to innovate quickly, release products fast, and satisfy investors, all of which may lead to shortcuts in testing, limited bias analysis, or insufficient reporting of demographic performance. Companies hold the technical knowledge that hospitals may not have, so they also have a responsibility to clearly communicate the limitations, risks, and performance characteristics of their products.

## 5. Government and Regulatory Bodies
Government agencies and medical regulators play a critical role in establishing rules for safe, ethical AI use in healthcare. These institutions ensure that:
	Medical devices meet safety standards
	Bias testing is performed before approval
	Companies disclose risk and performance data
	Patient data is protected
	Discriminatory outcomes are prevented

However, AI technology has evolved faster than regulatory frameworks. Many governments do not yet require companies to prove demographic fairness or disclose performance gaps by race or skin tone. Regulators must balance innovation with patient protection and have a responsibility to create standards that prevent biased AI from entering clinical practice. Their involvement ensures that the public interest—particularly the protection of vulnerable communities—is prioritized over commercial interests.


These are the stakeholders I am lookingforward to explore on my case study.
	**Patients** (especially darker-skin patients): need safe, fair diagnoses
    **Doctors and nurses**: need reliable systems to trust in decision-making
	**Hospitals**: want to use AI but must ensure patient safety
	**AI companies**: build tools and must fix bias issues
	**Government/regulators**: protect public health and fairness
	**Society**: wants equal access to accurate medical care



[← Back to Main Case study](/ethics_fall2025/casestudy)